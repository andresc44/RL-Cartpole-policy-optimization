{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XKeH_zTq5HYy"
      },
      "outputs": [],
      "source": [
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_ra1p0Q97e1C"
      },
      "outputs": [],
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VawMI5Pb3wJy",
        "outputId": "9a3a60e7-7955-4ac4-88cf-726ff4d3e510"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!apt-get install -y ffmpeg"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mountDrive = True\n",
        "if mountDrive:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "3Q9V-Y4bFYVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQ9xl5Rb7hQc"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "if not hasattr(np, \"bool8\"):\n",
        "    np.bool8 = np.bool_\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import animation\n",
        "from matplotlib.patches import Patch, FancyArrowPatch\n",
        "from IPython import display as ipythondisplay\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import pickle\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xr0Vb_U37jfz"
      },
      "outputs": [],
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(400, 300))\n",
        "display.start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JTBifkul7kGn"
      },
      "outputs": [],
      "source": [
        "# Naive Implementation with Random Forces applied\n",
        "env = gym.make('CartPole-v1', render_mode=\"rgb_array\", new_step_api=True)\n",
        "env.reset()\n",
        "prev_screen = env.render()\n",
        "plt.imshow(prev_screen[0])\n",
        "test_frames = []\n",
        "test_frames.append(prev_screen[0])\n",
        "\n",
        "for i in range(50000):\n",
        "  action = env.action_space.sample()\n",
        "  print(\"step i\",i,\"action=\",action)\n",
        "  obs, reward, done, truncated, info = env.step(action)\n",
        "  print(\"obs=\",obs,\"reward=\",reward,\"done=\",done,\"truncated=\", truncated,\"info=\",info)\n",
        "  screen = env.render()\n",
        "\n",
        "  plt.imshow(screen[0])\n",
        "  test_frames.append(screen[0])\n",
        "  ipythondisplay.clear_output(wait=True)\n",
        "  ipythondisplay.display(plt.gcf())\n",
        "\n",
        "  if done or truncated:\n",
        "    break\n",
        "\n",
        "ipythondisplay.clear_output(wait=True)\n",
        "env.close()\n",
        "\n",
        "print(\"Iterations that were run:\", i)\n",
        "print(\"obs=\",obs,\"reward=\",reward,\"done=\",done,\"truncated=\", truncated,\"info=\",info)\n",
        "\n",
        "\n",
        "if mountDrive:\n",
        "    with open(\"/content/drive/MyDrive/Academia&Work/projects/Masters/APS1080 (RL)/Exercises/frames_test.pkl\", \"wb\") as f:\n",
        "        pickle.dump(test_frames, f)\n",
        "    print(\"Model saved to frames_test.pkl\")\n",
        "\n",
        "    with open(\"/content/drive/MyDrive/Academia&Work/projects/Masters/APS1080 (RL)/Exercises/obs_test.pkl\", \"wb\") as f:\n",
        "        pickle.dump(obs, f)\n",
        "    print(\"Model saved to obs_test.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "scWo6T_SObsr"
      },
      "outputs": [],
      "source": [
        "def createVideo(vid_frames, true_obs, title):\n",
        "    r_obs = np.round(true_obs, 2)\n",
        "    print(f\"Pole reached critical tipping position at angle: {r_obs[2]*180/math.pi:.2f}\\u00B0,\"\n",
        "                          f\"\\nposition: {r_obs[0]:.2f}m, ang. vel.: {r_obs[3]*180/math.pi:.2f}\\u00B0/s, lin. vel.: {r_obs[1]:.2f} m/s\")\n",
        "    fps=8\n",
        "    hold_annotation=6\n",
        "    cart_centre = int(true_obs[0]*125.0+300)\n",
        "    print(f\"cart centre: {cart_centre}\")\n",
        "    frames_extended = vid_frames + [vid_frames[-1]] * fps*hold_annotation\n",
        "\n",
        "\n",
        "\n",
        "    # Save video\n",
        "    fig, ax = plt.subplots(figsize=(6, 4), dpi=400)\n",
        "    im = ax.imshow(vid_frames[0], interpolation='auto')\n",
        "    ax.axis('off')\n",
        "    ax.set_title(\"Cartpole Demo of REINFORCE Algorithm,\\nPolicy-based Learning\")\n",
        "    pole_color = np.mean(np.array([202, 152, 101]).reshape(-1, 3), axis=0) / 255\n",
        "\n",
        "    # Add legend\n",
        "    legend_elements = [\n",
        "        Patch(facecolor='black', edgecolor='black', label='Cart'),\n",
        "        Patch(facecolor=pole_color, label='Pole')\n",
        "    ]\n",
        "    ax.legend(handles=legend_elements, loc='upper right')\n",
        "    y_pos = 300\n",
        "    arrow = FancyArrowPatch(\n",
        "        (-5, y_pos),                        # start point\n",
        "        (vid_frames[0].shape[1]+5, y_pos),     # end point\n",
        "        arrowstyle='<->',\n",
        "        mutation_scale=15,\n",
        "        color='black',\n",
        "        linewidth=3\n",
        "    )\n",
        "    ax.add_patch(arrow)\n",
        "\n",
        "\n",
        "\n",
        "    def update(frame_idx):\n",
        "        im.set_array(frames_extended[frame_idx])\n",
        "\n",
        "        artists = [im]\n",
        "        # Add text after last real frame\n",
        "        if frame_idx >= len(vid_frames) - 1:\n",
        "            text = ax.text(vid_frames[0].shape[1]//2, 130,\n",
        "                          f\"Pole reached critical tipping position at angle: {r_obs[2]*180/math.pi:.2f}\\u00B0,\"\n",
        "                          f\"\\nposition: {r_obs[0]:.2f}m, ang. vel.: {r_obs[3]*180/math.pi:.2f}\\u00B0/s, lin. vel.: {r_obs[1]:.2f} m/s\",\n",
        "                          color='red', fontsize=10, ha='center', va='top',\n",
        "                          fontweight='light')\n",
        "            vline = ax.axvline(x=cart_centre,ymin=0.27, ymax=0.58,color='blue', linestyle=':', linewidth=2)\n",
        "            th_diff = 6 if true_obs[2] > 0 else -12\n",
        "            v_sign = 1 if true_obs[1] > 0 else -1\n",
        "            theta_text = ax.text((cart_centre+th_diff), 200, r'$\\theta$',\n",
        "                                color='blue', fontsize=8, fontweight='bold')\n",
        "            v_arrow = FancyArrowPatch(\n",
        "                posA=(cart_centre-v_sign*20, 300),\n",
        "                posB=(cart_centre+v_sign*25, 300),\n",
        "                arrowstyle='->',\n",
        "                color='yellow',\n",
        "                linewidth=2,\n",
        "                mutation_scale=10\n",
        "            )\n",
        "\n",
        "            ax.add_patch(v_arrow)\n",
        "            artists.extend([text, vline, theta_text, v_arrow])\n",
        "\n",
        "        return artists\n",
        "    ani = animation.FuncAnimation(fig, update, frames=len(frames_extended), interval=50, blit=True)\n",
        "    if mountDrive:\n",
        "        ani.save(\n",
        "            '/content/drive/MyDrive/Academia&Work/projects/Masters/APS1080 (RL)/Exercises/'+title,\n",
        "            writer='ffmpeg',\n",
        "            dpi=400,\n",
        "            bitrate=5000,\n",
        "            fps=fps\n",
        "        )\n",
        "    ani.save(\n",
        "        title,\n",
        "        writer='ffmpeg',\n",
        "        dpi=400,\n",
        "        bitrate=5000,\n",
        "        fps=fps\n",
        "    )\n",
        "\n",
        "    print(\"MP4 Video successfully saved\")\n",
        "    plt.close(fig)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L6dI8IZJUXC8"
      },
      "outputs": [],
      "source": [
        "createVideo(test_frames, obs,'RL_REINFORCE_CartpoleTest.mp4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "0SoIPZzeFUXA"
      },
      "outputs": [],
      "source": [
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, alpha):\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "        self.fc3 = nn.Linear(128, action_dim)\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = torch.relu(self.fc1(state))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return torch.softmax(x, dim=-1)\n",
        "\n",
        "    def predict(self, state):\n",
        "        state = torch.tensor(state, dtype=torch.float32)\n",
        "        action_probs = self.forward(state)\n",
        "        return action_probs.detach().numpy()\n",
        "\n",
        "    def gradient_log_prob(self, state, action):\n",
        "        state = torch.tensor(state, dtype=torch.float32)\n",
        "        action_probs = self.forward(state)\n",
        "        log_prob = torch.log(action_probs[action])\n",
        "        self.optimizer.zero_grad()\n",
        "        log_prob.backward()\n",
        "        return [param.grad for param in self.parameters()]\n",
        "\n",
        "    def update_policy(self, grad_log_prob, G, t, gamma):\n",
        "        for param, grad in zip(self.parameters(), grad_log_prob):\n",
        "            param.grad = -grad * (gamma ** t) * G\n",
        "        self.optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ERxqRUN3778q"
      },
      "outputs": [],
      "source": [
        "class REINFORCE:\n",
        "    def __init__(self, alpha, gamma, policy_network, num_episodes):\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.policy_network = policy_network\n",
        "        self.num_episodes = num_episodes\n",
        "        self.ep_lengths = []\n",
        "\n",
        "    def generate_episode(self, env):\n",
        "        episode = []\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        truncated = False\n",
        "        actions_taken = 0\n",
        "        while not (done or truncated):\n",
        "            action_probs = self.policy_network.predict(state)\n",
        "            action = np.random.choice(len(action_probs), p=action_probs)\n",
        "            next_state, reward, done, truncated, _ = env.step(action)\n",
        "            episode.append((state, action, reward))\n",
        "            state = next_state\n",
        "            actions_taken += 1\n",
        "        self.ep_lengths.append(actions_taken)\n",
        "        return episode, actions_taken\n",
        "\n",
        "    def reinforce(self, env):\n",
        "        longest_episode = 0\n",
        "        for episode in range(self.num_episodes):\n",
        "            episode_data, actions_taken = self.generate_episode(env)\n",
        "            if actions_taken > longest_episode:\n",
        "                longest_episode = actions_taken\n",
        "                print(f\"Longest episode so far: {longest_episode}\")\n",
        "                self.best_agent = copy.deepcopy(self)\n",
        "            for t in range(len(episode_data)):\n",
        "                G = 0\n",
        "                for k in range(t, len(episode_data)):\n",
        "                    _, _, reward = episode_data[k]\n",
        "                    G += (self.gamma ** (k - t)) * reward\n",
        "\n",
        "                state, action, _ = episode_data[t]\n",
        "                grad_log_prob = self.policy_network.gradient_log_prob(state, action)\n",
        "                self.policy_network.update_policy(grad_log_prob, G, t, self.gamma)\n",
        "\n",
        "            if (episode % 500 == 0) and mountDrive:\n",
        "              with open(\"/content/drive/MyDrive/Academia&Work/projects/Masters/APS1080 (RL)/Exercises/reinforce_agent.pkl\", \"wb\") as f:\n",
        "                pickle.dump(self, f)\n",
        "              print(f\"Episode {episode} completed, saved reinforce_agent checkpoint\")\n",
        "\n",
        "    def test_policy(self, version):\n",
        "        env = gym.make(\"CartPole-v1\", max_episode_steps=10_000, new_step_api=True, render_mode='rgb_array')\n",
        "        obs = env.reset()\n",
        "        prev_screen = env.render()\n",
        "        plt.imshow(prev_screen[0])\n",
        "        frames = []\n",
        "        frames.append(prev_screen[0])\n",
        "        ipythondisplay.display(plt.gcf())\n",
        "\n",
        "        for i in range(50000):\n",
        "            action_probs = self.policy_network.predict(obs)\n",
        "            action = np.argmax(action_probs)\n",
        "            obs, reward, done, truncated, info = env.step(action)\n",
        "\n",
        "            print(f\"Step {i}: Action={action}, Obs={obs}, Reward={reward}, Done={done}, Truncated={truncated}, Info={info}\")\n",
        "\n",
        "            screen = env.render()\n",
        "            plt.imshow(screen[0])\n",
        "            frames.append(screen[0])\n",
        "            ipythondisplay.clear_output(wait=True)\n",
        "            ipythondisplay.display(plt.gcf())\n",
        "\n",
        "            if done or truncated:\n",
        "                break\n",
        "\n",
        "        ipythondisplay.clear_output(wait=True)\n",
        "        env.close()\n",
        "        print(f\"Iterations that were run: {i}\")\n",
        "\n",
        "        if mountDrive:\n",
        "            with open(f\"/content/drive/MyDrive/Academia&Work/projects/Masters/APS1080 (RL)/Exercises/frames_{version}.pkl\", \"wb\") as f:\n",
        "                pickle.dump(frames, f)\n",
        "            print(\"Model saved to frames.pkl\")\n",
        "\n",
        "            with open(f\"/content/drive/MyDrive/Academia&Work/projects/Masters/APS1080 (RL)/Exercises/obs_{version}.pkl\", \"wb\") as f:\n",
        "                pickle.dump(obs, f)\n",
        "            print(\"Model saved to obs.pkl\")\n",
        "\n",
        "\n",
        "        createVideo(frames, obs,f'RL_REINFORCE_Cartpole_{version}.mp4')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dXECybjdFWZG"
      },
      "outputs": [],
      "source": [
        "# Set up the environment and hyperparameters\n",
        "env = gym.make(\"CartPole-v1\", max_episode_steps=10_000_000, new_step_api=True)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.n\n",
        "alpha = 0.01\n",
        "gamma = 0.99\n",
        "num_episodes = 10_000\n",
        "\n",
        "# Create the policy network\n",
        "policy_network = PolicyNetwork(state_dim, action_dim, alpha)\n",
        "\n",
        "# Create and run the REINFORCE algorithm\n",
        "reinforce_agent = REINFORCE(alpha, gamma, policy_network, num_episodes)\n",
        "reinforce_agent.reinforce(env)\n",
        "\n",
        "print(\"Model saved to reinforce_agent.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rkO87YgfFfNs"
      },
      "outputs": [],
      "source": [
        "# Test the trained policy\n",
        "# env = gym.make(\"CartPole-v1\", max_episode_steps=10_000, new_step_api=True, render_mode='rgb_array')\n",
        "# reinforce_agent.test_policy(\"latest_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Jaf7CR9mGfN"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"CartPole-v1\", max_episode_steps=10_000, new_step_api=True, render_mode='rgb_array')\n",
        "reinforce_agent.best_agent.test_policy(\"best_model\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}